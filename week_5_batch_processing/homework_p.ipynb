{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# **Week 5 - Homework**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "In this homework we'll put what we learned about Spark\n",
    "in practice.\n",
    "\n",
    "We'll use high volume for-hire vehicles (HVFHV) dataset for that.\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Question 1. Install Spark and PySpark**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "* Install Spark\n",
    "* Run PySpark\n",
    "* Create a local spark session \n",
    "* Execute `spark.version`\n",
    "\n",
    "What's the output?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import pyspark\n",
    "from pyspark.sql import SparkSession"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "spark = SparkSession.builder\\\n",
    "        .master(\"local[*]\")\\\n",
    "        .appName('test')\\\n",
    "        .getOrCreate()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "22/03/02 15:48:41 WARN Utils: Your hostname, ns3273592 resolves to a loopback address: 127.0.1.1; using 5.39.86.183 instead (on interface eno0)\n",
      "22/03/02 15:48:41 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "WARNING: An illegal reflective access operation has occurred\n",
      "WARNING: Illegal reflective access by org.apache.spark.unsafe.Platform (file:/home/ubuntu/.local/lib/python3.8/site-packages/pyspark/jars/spark-unsafe_2.12-3.2.1.jar) to constructor java.nio.DirectByteBuffer(long,int)\n",
      "WARNING: Please consider reporting this to the maintainers of org.apache.spark.unsafe.Platform\n",
      "WARNING: Use --illegal-access=warn to enable warnings of further illegal reflective access operations\n",
      "WARNING: All illegal access operations will be denied in a future release\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "22/03/02 15:48:45 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n",
      "22/03/02 15:48:51 WARN Utils: Service 'SparkUI' could not bind on port 4040. Attempting port 4041.\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "spark.version"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "'3.2.1'"
      ]
     },
     "metadata": {},
     "execution_count": 3
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **answer**  \n",
    "3.2.1"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Question 2. HVFHW February 2021**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Download the HVFHV data for february 2021:\n",
    "\n",
    "```bash\n",
    "wget https://nyc-tlc.s3.amazonaws.com/trip+data/fhvhv_tripdata_2021-02.csv\n",
    "```\n",
    "\n",
    "Read it with Spark using the same schema as we did \n",
    "in the lessons. \n",
    "\n",
    "Repartition it to 24 partitions and save it to\n",
    "parquet.\n",
    "\n",
    "What's the size of the folder with results (in MB)?\n"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "!wget https://nyc-tlc.s3.amazonaws.com/trip+data/fhvhv_tripdata_2021-02.csv"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "source": [
    "!wc -l fhvhv_tripdata_2021-02.csv"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "     1\t11613943 fhvhv_tripdata_2021-02.csv\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "source": [
    "!head -n 3 fhvhv_tripdata_2021-02.csv"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "source": [
    "from pyspark.sql import types"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "schema = types.StructType([\n",
    "    types.StructField('hvfhs_license_num',types.StringType(),True),\n",
    "    types.StructField('dispatching_base_num',types.StringType(),True),\n",
    "    types.StructField('pickup_datetime',types.TimestampType(),True),\n",
    "    types.StructField('dropoff_datetime',types.TimestampType(),True),\n",
    "    types.StructField('PULocationID',types.IntegerType(),True),\n",
    "    types.StructField('DOLocationID',types.IntegerType(),True),\n",
    "    types.StructField('SR_Flag',types.StringType(),True)\n",
    "])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "source": [
    "df = spark.read\\\n",
    "    .option(\"header\",\"true\")\\\n",
    "    .schema(schema)\\\n",
    "    .csv('fhvhv_tripdata_2021-02.csv')\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "source": [
    "df.show(5)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|hvfhs_license_num|dispatching_base_num|    pickup_datetime|   dropoff_datetime|PULocationID|DOLocationID|SR_Flag|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "|           HV0003|              B02764|2021-02-01 00:10:40|2021-02-01 00:21:09|          35|          39|   null|\n",
      "|           HV0003|              B02764|2021-02-01 00:27:23|2021-02-01 00:44:01|          39|          35|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:28:38|2021-02-01 00:38:27|          39|          91|   null|\n",
      "|           HV0005|              B02510|2021-02-01 00:43:37|2021-02-01 01:23:20|          91|         228|   null|\n",
      "|           HV0003|              B02872|2021-02-01 00:08:42|2021-02-01 00:17:57|         126|         250|   null|\n",
      "+-----------------+--------------------+-------------------+-------------------+------------+------------+-------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "df = df.repartition(24)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "df.write.parquet('fhvhv/2021/02/', mode='overwrite')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "!du -h fhvhv"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "421M\tfhvhv/2021/02\n",
      "421M\tfhvhv/2021\n",
      "421M\tfhvhv\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **answer**  \n",
    "~ 208MB"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Question 3. Count records** "
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "How many taxi trips were there on February 15?\n",
    "\n",
    "Consider only trips that started on February 15."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "source": [
    "from pyspark.sql import functions as F"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "source": [
    "df = spark.read.parquet('fhvhv/2021/02/*')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "source": [
    "df.registerTempTable('fhvhv')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "query = \"\"\"\n",
    "            SELECT\n",
    "                COUNT(*)\n",
    "            FROM\n",
    "                fhvhv\n",
    "            WHERE\n",
    "                date_format(pickup_datetime, 'yyyyMMdd') = '20210215'\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "spark.sql(query).show()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **answer**  \n",
    "367170"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Question 4. Longest trip for each day**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now calculate the duration for each trip.\n",
    "\n",
    "Trip starting on which day was the longest? "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "source": [
    "from pyspark.sql.functions import col, asc,desc"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "source": [
    "timeFmt = \"yyyy-MM-dd HH:mm:ss\"\n",
    "timeDiff = (F.unix_timestamp('dropoff_datetime', format=timeFmt)\n",
    "            - F.unix_timestamp('pickup_datetime', format=timeFmt))/60\n",
    "fhvhv_bis = df.withColumn(\"trip_duration\", timeDiff)\n",
    "\n",
    "fhvhv_bis.registerTempTable('fhvhv_bis')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "source": [
    "query = \"\"\"\n",
    "        SELECT date_format(pickup_datetime, 'yyyy-MM-dd')\n",
    "        FROM fhvhv_bis\n",
    "        WHERE trip_duration = (SELECT MAX(trip_duration) FROM fhvhv_bis) \n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+----------------------------------------+\n",
      "|date_format(pickup_datetime, yyyy-MM-dd)|\n",
      "+----------------------------------------+\n",
      "|                              2021-02-11|\n",
      "|                              2021-02-11|\n",
      "+----------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **answer**\n",
    "2021-02-11"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Question 5. Most frequent**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "`dispatching_base_num`\n",
    "\n",
    "Now find the most frequently occurring `dispatching_base_num` \n",
    "in this dataset.\n",
    "\n",
    "How many stages this spark job has?\n",
    "\n",
    "> Note: the answer may depend on how you write the query,\n",
    "> so there are multiple correct answers. \n",
    "> Select the one you have."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "source": [
    "query = \"\"\"\n",
    "            SELECT\n",
    "                dispatching_base_num,\n",
    "                COUNT(dispatching_base_num) AS nb\n",
    "            FROM\n",
    "                fhvhv_bis\n",
    "            GROUP BY\n",
    "                dispatching_base_num\n",
    "            ORDER BY\n",
    "                nb DESC\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(query).show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+--------------------+-------+\n",
      "|dispatching_base_num|     nb|\n",
      "+--------------------+-------+\n",
      "|              B02510|6467328|\n",
      "|              B02764|1931136|\n",
      "|              B02872|1765378|\n",
      "|              B02875|1370780|\n",
      "|              B02765|1119536|\n",
      "|              B02869| 859440|\n",
      "|              B02887| 644662|\n",
      "|              B02871| 624728|\n",
      "|              B02864| 623206|\n",
      "|              B02866| 622178|\n",
      "|              B02878| 610370|\n",
      "|              B02682| 606510|\n",
      "|              B02617| 549020|\n",
      "|              B02883| 503234|\n",
      "|              B02884| 489926|\n",
      "|              B02882| 464346|\n",
      "|              B02876| 431386|\n",
      "|              B02879| 420274|\n",
      "|              B02867| 401060|\n",
      "|              B02877| 397876|\n",
      "+--------------------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **answer**\n",
    "B02510"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Question 6. Most common locations pair**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Find the most common pickup-dropoff pair. \n",
    "\n",
    "For example:\n",
    "\n",
    "\"Jamaica Bay / Clinton East\"\n",
    "\n",
    "Enter two zone names separated by a slash\n",
    "\n",
    "If any of the zone names are unknown (missing), use \"Unknown\". For example, \"Unknown / Clinton East\". "
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "source": [
    "query_1 = \"\"\"\n",
    "            (SELECT concat(PULocationID, '|', DOLocationID) AS location_pair,\n",
    "            COUNT(concat(PULocationID, '|', DOLocationID)) as count_location_pair\n",
    "            FROM fhvhv_bis \n",
    "            GROUP BY concat(PULocationID, '|', DOLocationID) )\n",
    "\"\"\"\n",
    "spark.sql(query_1).createOrReplaceTempView(\"count_of_location_pair\")\n",
    "\n",
    "\n",
    "query_2 = \"\"\"\n",
    "            SELECT location_pair \n",
    "            FROM count_of_location_pair \n",
    "            WHERE count_location_pair = (\n",
    "                                            SELECT MAX(count_location_pair)\n",
    "                                            FROM count_of_location_pair\n",
    "                                        )\n",
    "\"\"\"\n",
    "spark.sql(query_2).show()"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "+-------------+\n",
      "|location_pair|\n",
      "+-------------+\n",
      "|        76|76|\n",
      "+-------------+\n",
      "\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": []
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **answer**  \n",
    "East New York / East New York"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## **Bonus question. Join type**"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "(not graded) \n",
    "\n",
    "For finding the answer to Q6, you'll need to perform a join.\n",
    "\n",
    "What type of join is it?\n",
    "\n",
    "And how many stages your spark job has?"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "> **answer**  \n",
    "inner join  \n",
    "The SQL query has 3 stages."
   ],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.10",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.10 64-bit"
  },
  "interpreter": {
   "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}